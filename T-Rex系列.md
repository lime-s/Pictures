# T-Rex系列



## T-Rex：Counting by Visual Prompting

### 摘要

​	我们介绍T-Rex，这是一个交互式物体计数模型，旨在首先检测然后计数任何物体。我们整合了视觉提示，将物体计数制定为一个开放集的物体检测任务。用户可以通过在参考图像上标记点或边界框来指定感兴趣的物体，然后T-Rex检测所有具有类似图案的物体。在T-Rex的视觉反馈的指导下，用户还可以通过对缺失或错误检测的物体进行提示来交互式地完善计数结果。T-Rex在几个类别无关的计数基准上取得了最先进的性能。为了进一步挖掘其潜力，我们建立了一个新的计数基准，包括不同的场景和挑战。定量和定性的结果都表明，T-Rex拥有特殊的零次计数能力。我们还介绍了T-Rex的各种实际应用场景，说明了它在视觉提示领域的潜力。

### 引言

​	计算机视觉目前正在经历一场以基础模型和多模态大语言模型为主的革命。这些模型在广泛的任务中表现出显著的性能，包括分割、物体检测、理解和生成。然而，在这些任务中，物体计数这一关键任务受到的关注相对较少。

​	物体计数，即估计图像中存在的具体物体数量的任务，在许多实际领域中需求量很大，如交通、农业、工业、生物等。现有的物体计数解决方案可以大致分为四种类型：

- **As density map regression task：**一个常见的方法是回归一个二维密度图，其总和被用作计数结果。虽然有效，但密度图的不太直观的可视化使得用户很难评估计数结果的准确性。
- **As  closed-set  detection  task：**另一个直接的解决方案是采用闭集检测器（如YOLO）来检测物体，其中检测到的目标框数量总和作为计数结果。然而，受固定类别的限制，这种方法需要为新的类别重新收集数据和重新训练，这既费时又费力。
- **As open-vocabulary detection task：**为了克服闭集检测方法的局限性，另一种方法是调整开放词汇检测器（如 `Grounding DINO`），通过文本提示检测任意物体。然而，计数的任务带来了巨大的挑战，因为许多物体没有简洁的文字描述，使得按文字指定物体变得困难。
- **As MLLM QA task：**多模态大型语言模型（MLLM）也可以通过问题回答用于物体计数。然而，多模式LLM中的幻觉问题会影响其计数结果的置信度，因为用户可能对MLLM的数字输出持怀疑态度，而没有额外的支持证据。

​	通过强调现有计数解决方案的局限性，我们认为一个实用的计数系统应该拥有以下四个特性：a) ==直观的视觉反馈：==它应该提供高度可解释的视觉反馈（如边界框），使用户能够验证计数结果的准确性。b) ==开放集：==它应该能够计算任何对象，不受预定义类别的限制。c) ==视觉可提示性：==考虑到文本对各种物体辨别的限制，它应该允许用户通过视觉例子来指定用于计数的物体。d) ==互动性：==它应该使用户能够积极参与计数过程，以纠正模型所犯的错误。

![image-20240326093713310](https://cdn.jsdelivr.net/gh/lime-s/Pictures@main/image-20240326093713310.png)

> 图二：T-Rex是一个物体计数模型，它的特点是有四个特征：基于检测、视觉提示、可交互和开放集

​	在这个设计理念的指导下，我们开发了一个基于检测的计数模型，称为T-Rex，如图2所示。用户可以通过在参考图像上标记方框或点来指定感兴趣的对象。反过来，T-Rex检测所有在目标图像中具有类似模式的实例，检测到的盒子的累积和代表计数结果。==有了T-Rex的视觉反馈，用户可以交互式地在遗漏或错误检测的物体上添加额外的提示。这个互动过程允许对T-Rex预测进行持续的完善，使用户能够自信地获得计数结果的准确性。值得注意的是，这个交互过程仍然是快速和资源高效的，因为**每一轮的交互只需要运行T-Rex的解码器**。==

​	T-Rex在两个计数基准上取得了最先进的结果。为了进一步衡量其潜力，我们引入了一个新的计数基准CA-44，它包括8个领域的44个数据集，呈现出多样化和艰巨的挑战。我们的实验结果表明，T-Rex拥有强大的零点计数能力，并能在各种情况下取得良好的性能。最后，我们探讨了T-Rex的广泛应用场景。凭借其多功能的计数能力和互动功能，T-Rex有可能对各个领域做出重大贡献，如零售、交通、农业、工业、牲畜、医学等。



### 方法

![image-20240326095915781](https://cdn.jsdelivr.net/gh/lime-s/Pictures@main/image-20240326095915781.png)

> 图三：T-Rex模型的概述。T-Rex是一个基于检测的模型，包括一个图像编码器来提取图像特征，一个提示编码器来编码用户提供的视觉提示（点或边界框），以及一个边界框解码器来输出检测到的边界框。

​	我们简单介绍一下T-Rex模型。T-Rex由三个部分组成，包括一个图像编码器、一个提示编码器和一个边界框解码器，如图3所示。给定一个目标图像输入 $I_{tgt}$ 和可选的参考图像输入 $I_{ref}$ （ 在没有单独的参考图像的情况下 ， 目标图像也可以作为参考图像 ） ， 图像编码器首先提取视觉特征 $E_{tgt}$, $E_{ref}$ 。然后，使用用户绘制的方框或点作为参考图像上目标物体的提示 $P$，提示编码器从参考图像特征 $E_{ref}$中提取编码的视觉提示 $P_{enc}$。最后，边界框解码器将目标图像特征 $E_{tgt}$ 和编码的视觉提示 $P_{enc}$ 结合起来作为输入，输出检测到的边界框 $B$ 及其相关的置信度分数 $S$​ 。一个预先确定的分数阈值被应用于过滤检测到的边界框，其余的边界框被加起来产生最终的物体数量。

![image-20240326100612922](https://cdn.jsdelivr.net/gh/lime-s/Pictures@main/image-20240326100612922.png)



#### 工作流

![image-20240326100833929](https://cdn.jsdelivr.net/gh/lime-s/Pictures@main/image-20240326100833929.png)

> 图四：T-Rex提供了三个主要的交互式工作流程，适用于现实世界应用中的大多数场景。

​	T-Rex提供了三个主要的交互式工作流程，如图4所示。我们在下面解释每个工作流程及其应用：

- **Positive-only Prompt Mode：**在大多数计数场景中，用户通常只需要点击一次或画一个盒子，T-Rex可以有效地检测所有具有类似模式的物体。然而，在涉及密集和小物体的情况下，单轮提示可能是不够的。在这种情况下，用户可以根据T-Rex的视觉反馈，选择在遗漏的区域加入额外的提示。这种迭代细化的方法可以得到更准确的计数结果。
- **Positive  with  Negative  Prompt  Mode：**在存在其他类似物体干扰的情况下，T-Rex可能会产生错误的检测框。如图4所示，当提示指向一个绿苹果时，由于这两种物体类型之间有很强的几何相似性，T-Rex可能会错误地检测到橙子。在这种情况下，用户可以通过向错误检测的对象添加负面提示来纠正计数结果。
- **Cross-Image Prompt Mode：**T-Rex还提供了对跨图像计数的支持，允许不同的参考和目标图像的组合。这一功能在自动注释场景中被证明特别有用，用户只需要在一张图片上提示一次，而T-Rex可以自动注释其他表现出与提示图片相似的对象模式的图片。



#### Discussion

​	T-Rex本质上是一个开放集的物体检测模型。与依赖基于文本的提示的开放词汇对象检测器相比，T-Rex采用了视觉提示。由于在许多现实世界的计数应用中，文本描述可能无法充分捕捉到所有物体的细节，因此采用视觉提示提供了一个更直接和通用的选择。

​	在物体计数任务的背景下，最重要的考虑是围绕着对模型的高度可靠预测的需要。鉴于计数结果以统计值表示，即使预测值有微小的差异，也意味着计数失败。因此，我们将T-Rex设计成交互式的，允许用户根据视觉反馈反复纠正计数结果，从而提高计数的准确性。关于模型的结构，T-Rex只需要通过图像编码器进行一次前向传递，而随后的几轮互动只涉及提示编码器和盒子解码器。这种简化的方法确保了整个交互过程保持轻量级和快速。



### 结论

​	在本文中，我们介绍了T-Rex，一个用于交互式物体计数的创新模型，其特点是能够利用视觉提示检测和计数物体。T-Rex代表了计算机视觉中视觉提示方法的重大进步，与NLP中观察到的成功相类似，LLM通过文本提示促进Human-AI的互动。这种平行关系表明有很大的可能性，视觉提示在计算机视觉中的应用可以预示着像NLP中的类似突破。





## T-Rex2：Towards Generic Object Detection via Text-Visual Prompt Synergy

### 摘要	

​	我们提出了T-Rex2，一个高度实用的开放集物体检测模型。以前依靠文本提示的开放集物体检测方法有效地封装了常见物体的抽象概念，但由于数据的稀缺性和描述性的限制，在罕见或复杂的物体表示方面很吃力。相反，视觉提示在通过具体的视觉例子来描述新的物体方面表现出色，但无法像文字提示那样有效地传达物体的抽象概念。认识到文本和视觉提示的互补优势和劣势，我们引入了T-Rex2，通过对比学习在一个单一的模型中协同两种提示。T-Rex2接受不同格式的输入，包括文本提示、视觉提示和两者的结合，这样它就可以通过在两种提示模式之间的切换来处理不同的场景。综合实验表明，T-Rex2在广泛的场景中表现出显著的零样本物体检测能力。我们表明，文本提示和视觉提示可以在协同作用中相互受益，这对于覆盖大量复杂的现实世界场景至关重要，并为通用物体检测铺平道路。



### 引言

​	物体检测是计算机视觉的一个基础支柱，旨在定位和识别图像中的物体。传统意义上，物体检测是在封闭集范式下进行的，其中预先定义的类别集是已知的，系统被训练为从这个类别集中识别和检测物体。然而，现实世界的不断变化的和不可预见的性质要求物体检测方法向开放式范式转变。

​	开放集物体检测代表了一个重大的范式转变，通过使模型能够识别超出预定类别集的对象克服了封闭集检测的限制。一个普遍的方法是使用文本提示进行开放词汇的物体检测。这种方法通常涉及从CLIP[32]或BERT[3]等语言模型中提炼知识，使文本描述与视觉表征相一致。

![image-20240326112118673](https://cdn.jsdelivr.net/gh/lime-s/Pictures@main/image-20240326112118673.png)

> 图二：

​	虽然使用文本提示因其抽象描述对象的能力而在开集检测中受到主要青睐，但它仍然面临以下限制：

1. **Long-tailed data shortage：**文本提示的训练需要与视觉表示进行模态对齐，然而，长尾对象数据的稀缺可能会损害学习效率。 如图 2 所示，物体的分布本质上遵循长尾模式，即随着可检测物体种类的增加，这些物体的可用数据变得越来越稀缺。 这种数据稀缺可能会削弱模型识别稀有或新颖物体的能力。 
2. **Descriptive limitations：** 文本提示也无法准确描述难以用语言描述的对象。 例如，如图2所示，虽然文本提示可以有效地描述摩天轮，但如果没有生物学知识，它可能很难准确地表示显微镜图像中的微生物。

​	相反，视觉提示提供了一种更直观和直接的方法，通过提供视觉例子来表示物体。例如，用户可以使用点或边界框来标记物体进行检测，即使他们不知道物体是什么。此外，视觉提示不受跨模态对齐需求的限制，因为它们依赖于视觉相似性而不是语言相关性，使它们能够应用于训练期间没有遇到的新物体。

​	尽管如此，视觉提示也表现出局限性，因为与文本提示相比，它们在捕捉物体的一般概念方面不太有效。例如，作为文本提示的 “狗” 这个词广泛涵盖了所有的狗的品种。相比之下，鉴于狗的品种、大小和颜色的巨大多样性，视觉提示需要一个全面的图像集来直观地传达狗的抽象概念。

​	认识到文本和视觉提示的互补优势和劣势，我们提出了T-Rex2，一个整合了两种模式的通用开放集物体检测模型。T-Rex2是基于DETR架构的端到端目标检测模型。它结合了两个平行编码器来编码文本和视觉提示。对于文本提示，我们利用CLIP[32]的文本编码器将输入文本编码为文本嵌入。对于视觉提示，我们引入了一个新的视觉提示编码器，配备了可变形注意力[55]，可以将单一图像或跨多个图像的输入视觉提示（点或边界框）转化为视觉提示嵌入。为了促进这两种提示模式的合作，我们提出了一个对比学习[9, 32]模块，可以明确地对齐文本提示和视觉提示。**在对齐过程中，视觉提示可以从文本提示中固有的通用性和抽象性能力中受益。相反，文本提示可以通过查看各种视觉提示来增强其描述能力。**这种反复的互动使视觉和文本提示不断发展，从而提高了它们在一个模型中的通用理解能力。

​	T-Rex2支持四个独特的工作流程，可以应用于各种场景。1）交互式视觉提示工作流程，允许用户通过框或点在当前图像上指定要检测的对象；2）通用视觉提示工作流程，允许用户通过视觉提示在多个图像中定义一个特定的对象，从而创建一个适用于其他图像的通用视觉嵌入；3）文本提示工作流程，使用户能够采用描述性文本进行开放词汇对象检测；4）混合提示工作流：它结合了文本和视觉提示进行联合推理。

​	T-Rex2展示了强大的物体检测能力，并在COCO[20]、LVIS[8]、ODinW[15]和Roboflow100[2]上取得了显著的效果，这些都是在零样本的情况下。通过我们的分析，我们观察到文本和视觉提示起到了互补的作用，在另一个提示可能不那么有效的情况下，它们各自表现出色。具体来说，==文本提示在识别常见物体方面表现出色，而视觉提示在罕见的物体或可能不容易通过语言描述的场景中表现出色。==这种互补关系使该模型能够在广泛的场景中有效地执行。总而言之，我们的贡献有三个方面：

1. 我们提出了一个开放集的物体检测模型T-Rex2，它将文本和视觉提示统一在一个框架内，它在各种场景中表现出强大的零样本能力。
2. 我们提出了一个对比性学习模块来明确地对齐文本和视觉提示，这使这两种模式的相互增强
3. 广泛的实验证明了在一个模型中统一文本和视觉提示的好处。我们还揭示了每种类型的提示都可以涵盖不同的场景，这些场景共同显示了向一般物体检测发展的前景。



### 模型

![image-20240326153424275](https://cdn.jsdelivr.net/gh/lime-s/Pictures@main/image-20240326153424275.png)

​	T-Rex2整合了四个部分，如图3所示。i）图像编码器，ii）视觉提示编码器，iii）文本提示编码器，和iv）盒子解码器。T-Rex2坚持DETR[1]的设计原则，DETR是一个端到端的物体检测模型。这四个组件共同促进了四个不同的工作流程，包含了广泛的应用场景。

#### Visual-Text Promptable Object Detection

**Image  Encoder：**与可变形 `DETR` 框架相似，T-Rex2中的图像编码器由一个视觉骨干（如 `Swin Transformer`）组成，从输入图像中提取多尺度特征图。随后是几个配备了可变形自注意的 `Transformer` 编码器层，利用它们来完善这些提取的特征图。图像编码器输出的特征图表示为 $f_i \in R^{C_i \texttimes H_i \texttimes W_i}$，$i \in \{1,2,...,L\}$，其中 L 是特征图层的数量。

**Visual Prompt Encoder：**视觉提示已被广泛用于交互式分割，但在物体检测领域还没有得到充分的探索。我们的方法结合了边界框和点格式的视觉提示。设计原则包括将用户指定的视觉提示从其坐标空间转换到图像特征空间。给定 K 个用户指定的 4D 归一化边界框 $b_j = (x_j ,y_j ,w_j ,h_j ),j \in \{1,2,...,K\}$，或参考图像上的 2D 归一化点 $p_j = (x_j ,y_j ),j \in \{1,2,...,K\}$，我们最初通过一个固定的正余弦嵌入层将这些坐标输入编码为位置嵌入。随后，采用两个不同的线性层将这些嵌入投射到一个统一的维度上。

![image-20240326155951989](https://cdn.jsdelivr.net/gh/lime-s/Pictures@main/image-20240326155951989.png)

​	其中，PE 代表位置嵌入，$\text{Linear}(·, \theta)$ 表示参数为 $\theta$ 的线性投影操作。与之前的方法不同（将点视为宽度和高度最小的边界框），我们将边界框和点建模为不同的提示类型。然后我们初始化一个可学习的内容嵌入，广播K次，表示为 $C \in {R^{K \texttimes D}}$ 。此外，一个通用的类标记 $C'\in R^{1\texttimes D}$​被用来聚合来自其他视觉提示的特征，以适应用户可能在一张图像中提供多个视觉提示的情况。这些内容嵌入与沿通道维度的位置嵌入堆叠起来，并应用线性层进行投影，从而构建输入查询嵌入 $Q$ ：

![image-20240326160727932](https://cdn.jsdelivr.net/gh/lime-s/Pictures@main/image-20240326160727932.png)

​	其中，$\text{CAT}$ 表示通道维度上的堆叠操作。$B' 和 P'$ 代表全局位置嵌入，他们源于归一化坐标 $[0.5, 0.5, 1, 1] 和 [0.5， 0.5]$。全局查询的目的是聚合其他查询的特征。 随后，我们采用多尺度可变形交叉注意力层，以视觉提示为条件，从多尺度特征图中提取视觉提示特征。 对于第 j 个提示，交叉注意后的查询特征 $Q'_j$ 计算为：

![image-20240326162555161](https://cdn.jsdelivr.net/gh/lime-s/Pictures@main/image-20240326162555161.png)

​	可形变注意力最初被用来解决DETR中遇到的缓慢收敛问题。 在我们的方法中，我们将可变形注意力放在视觉提示的坐标上，即每个查询将选择性地关注包含视觉提示周围区域的一组有限的多尺度图像特征。 这确保了捕获代表感兴趣对象的视觉提示嵌入。 在提取过程之后，我们使用自注意力层来调节不同查询之间的关系，并使用前馈层进行投影。 全局内容查询的输出将作为最终的视觉提示嵌入V：

![image-20240326165613897](https://cdn.jsdelivr.net/gh/lime-s/Pictures@main/image-20240326165613897.png)

**Text Prompt Encoder：**我们采用CLIP的文本编码器对类别名称或短语进行编码，并使用[CLS]标记输出作为文本提示嵌入，表示为T。

**Box Decoder：**我们采用一个类似DETR的解码器进行边界框预测。按照 `DINO`，每个查询被表述为一个4D锚框坐标，并在解码器层中进行迭代细化。我们采用 `Grounding DINO` 中提出的查询选择层来初始化锚点坐标$(x,y,w,h)$。具体来说，我们计算编码器特征和提示嵌入之间的相似度，并选择相似度为前900的索引来初始化位置嵌入。随后，检测查询利用可变形交叉注意来关注编码的多尺度图像特征，并用于预测每个解码层的锚点偏移量 $(\Delta x,\Delta y,\Delta w,\Delta h)$。最终的预测框是由锚点(框)和偏移量相加得到的。

![image-20240327114113876](https://cdn.jsdelivr.net/gh/lime-s/Pictures@main/image-20240327114113876.png)

​	其中 $Q_{dec}$ 是来自边界框解码器的预测查询。按照以前的开放集物体检测方法[19, 24]，我们==不使用可学习的线性层来预测类标签，而是利用提示嵌入作为分类层的权重==。

![image-20240327114332482](https://cdn.jsdelivr.net/gh/lime-s/Pictures@main/image-20240327114332482.png)

​	其中 $C$ 表示视觉提示类的总数，$N$ 表示检测查询的数量。视觉提示对象检测和开放词汇对象检测任务都共享相同的图像编码器和边界框解码器。

#### Region-Level Contrastive Alignment

​	为了在一个模型中整合视觉提示和文本提示，我们采用了区域级的对比学习来对齐这两种模态。具体来说，给定一个输入图像和 $K$ 个视觉提示嵌入 $V = (v_1,...,v_K)$ 从视觉提示编码器中提取，同时还有文本提示嵌入 $T=(t_1,...,t_K)$ 对于每个提示区域，我们计算两类嵌入之间的InfoNCE损失[30]。

![image-20240327144745804](https://cdn.jsdelivr.net/gh/lime-s/Pictures@main/image-20240327144745804.png)

​	对比性对齐可以被看作是一个相互提炼的过程，据此，每种模式都有助于知识的交流并从中受益。具体来说，文本提示可以被看作是一个概念锚，围绕着这个概念锚框，不同的视觉提示可以汇聚在一起，这样视觉提示就可以获得一般的知识。相反，视觉提示作为文本提示的一个连续的细化来源。通过接触一系列的视觉实例，文本提示被动态地更新和增强，获得了深度和细微的差别。
